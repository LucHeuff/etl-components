# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['sql_autoloader']

package_data = \
{'': ['*']}

install_requires = \
['more-itertools>=10.5.0,<11.0.0',
 'networkx==3.4.2',
 'polars>=1.9.0,<2.0.0',
 'psycopg>=3.2.3,<4.0.0',
 'pydantic>=2.9.2,<3.0.0']

setup_kwargs = {
    'name': 'sql-autoloader',
    'version': '2.0.0',
    'description': 'Components to automate loading steps in ETL pipelines using SQL',
    'long_description': '# SQL Autoloader\n\nContains automation of loading data into a SQL-like database, built around [`polars`](https://pola.rs/).\n\nAlso includes convience functions for semi-manually inserting and retrieving data.\nCurrently, `SQLite` (through `sqlite3`) and `PostgreSQL` (through `psycopg`) are supported.\n\n# Installation\n\n## `pip`\nInstall with `pip` using:\n\n```\npip install git+https://github.com/LucHeuff/sql-autoloader.git\n```\n## `poetry`\nInstall with [`poetry`](https://python-poetry.org) using:\n\n```\npoetry install git+https://github.com/LucHeuff/sql-autoloader.git\n```\nor by adding it to `pyproject.toml`:\n\n```\n[tool.poetry.dependencies]\nsql-autoloader = { git = "https://github.com/LucHeuff/sql-autoloader.git"}\n```\n\n# How does it work?\n\nOften, loading data into a SQL database consists of two basic steps that are repeated many times:\n\n1. `INSERT`ing data into a table\n2. `RETRIEVE`ing the primary keys the database associated with the inserted data.\n\nIf done manually, this requires either writing repetitive `INSERT` and `RETRIEVE` queries by hand, \nor writing some object-oriented replica of your database schema using an ORM and letting that handle \nthe loading steps (but this requires recording the database schema in two separate places).\n\nBoth of these work fine as long as the database schema does not change, but become labourious to edit\nonce the schema does change.\n\n`sql-autoloader` simplifies this process by being aware of the database schema, and automatically generating \nthe required `INSERT` and `RETRIEVE` SQL queries based on the data that you are trying to load. \n`sql-autoloader` does this by trying to match column names in the data with column names in the database schema,\nand also figures out the order in which to load such that consistency across referencing tables is maintained.\n\nThis does mean that `sql-autoloader` needs to make assumptions.\n\n## Assumptions\n- The data do not contain any missing values.\n    During the `RETRIEVE` step, data retrieved from the database are joined to the original data.\n    Missing values can behave very unpredictably during this process, so these are not allowed.\n- The database schema is defined prior to loading.\n    `sql-autoloader` reads the schema from the database, and tries to match this with the data you want to load.\n    That means the schema must be already be defined at the time of loading. \n- There are no loops in the database schema.\n    Internally, the schema is assumed to form a Directed Acyclic Graph, meaning that there are no cycles of tables\n    that reference each other in a loop.\n- Foreign keys are named consistently.\n    As far as I am aware, SQL does not require foreign keys referring to the same primary key in another table to have the same name.\n    However, this makes algorithmically figuring out the order in which tables should be loaded much more difficult,\n    so `sql-autoloader` requires all foreign keys that refer to the same primary key to have the same name. \n\n> Note that `sql-autoloader` will automatically raise exceptions if these assumptions are not met.\n\n## Validation\nBy default, `sql-autoloader` will try to validate the loading operation by retrieving all the data it loaded\nand comparing that to the original data provided by the user. If these data do not match, all changes to the database are rolled back.\nAutomatically generating the comparison query comes with an additional assumption: \n\n-  All tables on which data is loaded are connected.\n    This means there exists a single query consisting of multiple `JOIN` statements that reconstruct the original data.\n    That also means there can be no isolated tables, or sets of isolated tables in the loading operation.\n\n> Note that this doesn\'t mean that all your tables need to be connected, this only needs to hold for the tables into which data are loaded.\n\nIf your loading operation does match this assumption, you can either provide your own comparison query (through the `compare_query=` argument)\nor disable the validation entirely (by setting `compare=False`).\n\nThe automatically generated query can also be restricted by adding a `WHERE`-clause (through the `where=` argument),\nor relaxed by setting `exact=False`, meaning that instead of having to exactly match, the original data only needs to appear in the data retrieved from the database.\n\n# How do I use it?\n\n`sql-autoloader` provides a context manager for each supported database. For example:\n\nSQLite:\n```\nfrom sql_autoloader import SQLiteConnector\n\ncredentials = \'<path_to_file>.db\'\n\nwith SQLiteConnector(credentials) as sqlite:\n   sqlite.load(data) \n\n```\n\nPostgres:\n```\nfrom sql_autoloader import PostgresConnector\n\ncredentials = \'postgresql://<username>:<password>@<host>:<port>/<db_name>\'\n\nwith PostgresConnector(credentials) as postgres:\n    postgres.load(data)\n\n```\n\nThe context manager handles opening and closing the connection, and will roll back any changes on the database if an error occurs.\nIn addition, it will also create a cursor and expose it (e.g. `sqlite.cursor` exposes a `sqlite3.Cursor`, `postgres.cursor` exposes a `psycopg.Cursor`) \nin case you need access to the cursor directly.\n\nFor example, this can be useful when you want to create the database schema from within Python:\n\n```\nfrom sql_autoloader import SQLiteConnector\n\nschema = """\nCREATE TABLE demo (\n    id INTEGER PRIMARY KEY,\n    name TEXT UNIQUE\n);\n...\n\n"""\n\ncredentials = \'<path_to_file>.db\'\n\nwith SQLiteConnector(credentials) as sqlite:\n   sqlite.cursor.executescript(schema) \n   sqlite.update_schema() # The schema changed since when the context manager was created, so we need to update\n\n   sqlite.load(data) \n```\n\n> Note\n> All connectors assume `data` to be a `polars.DataFrame`. If you are coming from `pandas` instead,\n> you can easily convert your `pandas.DataFrame` to a `polars.DataFrame` using:\n```\nimport polars as pl\npolars_df = pl.from_pandas(pandas_df)\n```\n\n# Documentation\n\n## Connector\nAll `Connector`s have the following methods:\n\n### `load`\nThis is the main intended way for `sql-autoloader` to be used, which tries to automatically load the provided data\n```\nload(\n    data: pl.DataFrame,\n    columns: dict[str, str] | None = None,\n    compare: bool = True,\n    compare_query: str | None = None,\n    replace: bool = True,\n    allow_duplication: bool = False,\n    where: str | None = None,\n    exact: bool = True,\n) -> pl.DataFrame\n```\n`data`: a `polars.DataFrame` containing data to be loaded into the database\\\n`columns` (Optional): Translation of columns in the data to the relevant column names in the database. If the same column name appears on multiple tables in the database, prefix the column with the desired table using the format `\\<table\\>.\\<column_name\\>`\\\n`compare` (Optional): whether comparison needs to be performed\\\n`compare_query` (Optional): allows you to provide a custom comparison query for data validation. This is ignored when `compare=False`\\\n`replace` (Optional): Whether columns can be replaced with the relevant foreign keys upon retrieval. If set to `False`, all columns are preserved\\\n`allow_duplication` (Optional): Whether rows are allowed to be duplicated upon retrieval\\\n`where` (Optional): allows adding a `WHERE`-clause to be added onto the comparison query. Please prefix the column you are conditioning on with its relevant table, otherwise this condition may result in SQL errors\\\n`exact` (Optional): whether the rows in the data retrieved through the comparison query must match `data` exactly. If set to `False`, will only check if the rows from `data` appear in the retrieved data\\\n\nThis function will return the original data including the foreign keys (where original columns were replaced depending on `replace`), in case you want to use these downstream.\n\nIf for some reason `load()` does not produce the desired results, the loop can be constructed manually using the\n`insert()`, `retrieve_ids()` or `insert_and_retrieve_ids()` methods.\n\n### `insert`\nThis method inserts provided data into a single table. This can be used manually if `load()` is not working as desired.\n```\ninsert(\n    data: pl.DataFrame,\n    table: str,\n    columns: dict[str, str] | None = None,\n) -> None:\n```\n`data`: a `polars.DataFrame` containing data to be loaded into the table\\\n`table`: the table that the data should be loaded into\\\n`columns` (Optional): Translation of columns in the data to the relevant column names in the table, in the format {<data_name>: <db_name>}\\\n\nAs insertion is an operation on the database only, this method does not return anything.\n\n> Note that any columns that are present in `data` that are not relevant to `table` are simply ignored.\n\n### `retrieve_ids`\nThis methods retrieves primary keys from a single table and joins them to the provided data under the given alias.\n```\nretrieve_ids(\n    data: pl.DataFrame,\n    table: str,\n    alias: str,\n    columns: dict[str, str] | None = None,\n    replace: bool = True,\n    allow_duplication: bool = False,\n) -> pl.DataFrame:\n```\n`data`: a `polars.DataFrame` containing data to which the keys should be joined\\\n`table`: the table from which the primary keys should be retrieved\\\n`alias`: the alias under which the primary key should be retrieved. Usually this is the name of the foreign key in some other table, referring to this table\\\n`columns` (Optional): Translation of columns in the data to the relevant column names in the table, in the format {<data_name>: <db_name>}\\\n`replace` (Optional): Whether columns can be replaced with the relevant foreign keys upon retrieval. If set to `False`, all columns are preserved\\\n`allow_duplication` (Optional): Whether rows are allowed to be duplicated upon retrieval\\ \n\nThis method will return a dataframe onto which the primary keys of `table` were joined, under the provided `alias`.\n\n> Note that any columns that are present in `data` that are not relevant to `table` are simply ignored.\n\n### `insert_and_retrieve_ids`\nThis is a convenience method that chains `insert()` and `retrieve_ids()` for the same table.\n```\ninsert_and_retrieve_ids(\n    data: pl.DataFrame,\n    table: str,\n    alias: str,\n    columns: dict[str, str] | None = None,\n    replace: bool = True,\n    allow_duplication: bool = False,\n) -> pl.DataFrame:\n```\n*For parameter and output specification refer to `insert()` and `retrieve_ids()` above*\n\n### `compare`\nThis method performs comparison between the provided data and data fetched from the database using a provided query\n```\ncompare(\n    data: pl.DataFrame,\n    query: str | None = None,\n    columns: dict[str, str] | None = None,\n    where: str | None = None,\n    exact: bool = True,\n) -> None:\n```\n`data`: a `polars.DataFrame` containing data against which should be compared\\\n`query` (Optional): a `SELECT` query to be run against the database, to fetch data that should be compared to `data`\\\n    If left empty, the method will attempt to generate a comparison query automatically\\\n`columns` (Optional): Translation of columns in the data to the relevant column names in the table, in the format {<data_name>: <db_name>}\\\n`where` (Optional): a `WHERE` clause to filter selection from the database. Should always use table prefixes for the columns being conditioned on\\\n                     Mostly intended when `query` is left empty, otherwise you could just bundle it there as well\\\n`exact` (Optional): whether the rows in the data retrieved through the comparison query must match `data` exactly. If set to `False`, will only check if the rows from `data` appear in the retrieved data\\\n\n### `update_schema`\nThe database schema is retrieved whenever the `*Connector` context manager is created.\nHowever, you may wish to create or adjust the database schema from within the context manager itself, at which point the\nschema in the database and the schema in the `*Connector` are out of sync.\n`update_schema()` allows you to update the schema in the `*Connector`.\n\nFor example:\n```\nfrom sql_autoloader import SQLiteConnector\n\nschema = "<some valid SQL schema>"\n\n# creating a context manager on a new database file, so it is empty at the start\nwith SQLiteConnector("new.db") as sqlite:\n    # at this point, the schema representation is empty\n    sqlite.cursor.executescript(schema)\n    # schema inside the SQLite database is now updated, but the schema representation is still empty\n    sqlite.update_schema() # update the schema representation as well.\n    \n```\n\n### `print_schema`\n`print_schema()` is a convenience function to show a list of tables and the names of columns that the `*Connector` knows.\nThis is not intended as a replacement of the full SQL schema, but instead as a reference to quickly check if everything is in working order,\nor if you don\'t have access to the full SQL schema for some reason.\n\nfor example:\n```\nwith SQLiteConnector(credentials) as sqlite:\n    sqlite.print_schema()\n```\n\n> Note that the information is incomplete, as the `*Connector` is not aware of table and column constraints, or default values.\n\n## SQLiteConnector\nThe `SQLiteConnector` wraps the `sql-autoloader` functionality around the `sqlite3` library.\n```\nSQLiteConnector(\n    credentials: str,\n    allow_custom_dtypes: bool = False\n)\n```\n`credentials`: path to a `sqlite` database, or \':memory:\' for a SQLite database existing only in memory\\\n`allow_custom_dtypes` (Optional): enables custom datatypes, and can be used in combination with custom adapters and converters. For more information see the [sqlite3 documentation](https://docs.python.org/3/library/sqlite3.html#sqlite3-adapter-converter-recipes)\n\nThe `SQLiteConnector.cursor` property exposes the `sqlite3.Cursor` used internally for manual use. See the [sqlite3 documentation on Cursors](https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor) for more information.\n> Note: the `SQLiteConnector` assumes that the cursor will be closed once the context manager exits. Closing the cursor prematurely will cause issues.\n\n## PostgresConnector\nThe `PostgresConnector` wraps the `sql-autoloader` functionality around the `psycopg` library.\n```\nPostgresConnector(\n    credentials: str\n)\n```\n`credentials`: a [connection string](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING) to a running PostgreSQL server\\\n\nThe `PostgresCursor.cursor` property exposes the `psycopg.Cursor` used internally for manual use. See the [psycopg documentation on Cursors](https://www.psycopg.org/psycopg3/docs/api/cursors.html) for more information.\n> Note: the `PostgresConnector` assumes that the cursor will be closed once the context manager exits. Closing the cursor prematurely will cause issues.\n\n## Troubleshooting\nSince the `load()` operation has a lot of moving parts, troubleshooting can be difficult.\nFor that reason, the basic load operations write what they are trying to do, and the SQL query they are trying to execute\nto the debugging logs.\nThese can be accessed using the builting `logging` module by setting the level to `logging.DEBUG`, for instance:\n```\nimport logging\nlogging.getLogger("sql_autoloader").setLevel(level=logging.DEBUG) \n```\n\n',
    'author': 'Luc Heuff',
    'author_email': '10941592+LucHeuff@users.noreply.github.com',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.12,<4.0',
}


setup(**setup_kwargs)

